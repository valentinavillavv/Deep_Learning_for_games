{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "yH5RIEmq0-Gl"
      },
      "source": [
        "# Deep Reinforcement Learning\n",
        "In this lab we will implement and train an agent that uses deep learning to play balance the stick in `CartPole-v1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "bB8eAcnB0-Gs"
      },
      "source": [
        "## Setup\n",
        "----\n",
        "We import useful packages: `gym`, `torch` stuff, etc.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wUTgGqpl0-Gt"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PJ6ZckW00-Gu"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque  # for memory\n",
        "from tqdm import tqdm          # for progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nZjjJaER0-Gx"
      },
      "source": [
        "How the game looks (without our agent):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxMMj74S0-Gy",
        "outputId": "c499bcaf-bedf-4275-9757-1b6219a1f3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "100%|██████████| 10/10 [00:05<00:00,  1.68it/s]\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1', render_mode='human')\n",
        "for _ in tqdm(range(10)):\n",
        "    state, _,_,_ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PIalxc3I0-Gz"
      },
      "source": [
        "## DQN Algorithm\n",
        "-------------\n",
        "We train a policy that tries to maximize the discounted,\n",
        "cumulative reward\n",
        "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
        "$R_{t_0}$ is *return*. The discount, $\\gamma$ is the discount, between $0$ and $1$\n",
        "\n",
        "\n",
        "Q-learning tries to find a function\n",
        "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, maximizes rewards:\n",
        "\n",
        "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
        "\n",
        "However, we don't know $Q^*$. So, we use neural network as a approximators, we can simply create one and train it to resemble $Q^*$.\n",
        "\n",
        "For our training update rule, we'll use a fact that every $Q$\n",
        "function for some policy obeys the Bellman equation:\n",
        "\n",
        "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
        "\n",
        "The difference between the two sides of the equality is known as the temporal difference error, $\\delta$:\n",
        "\n",
        "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "29jwg8Re0-G1"
      },
      "source": [
        "### Model\n",
        "---\n",
        "Make a deep learning based policy model, that takes in a state and outputs an action.\n",
        "This model will be an attribute of the Agent we make next.\n",
        "Think about the activation function you use at the last layer, which should depends the type of action you want out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "p84SHMg90-G3"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, observation_size, action_size):\n",
        "    super(Model, self).__init__()\n",
        "    # initialise layers here\n",
        "    self.lvl1 = nn.Linear(observation_size, 64)\n",
        "    self.lvl2 = nn.Linear(64, 32)\n",
        "    self.lvl3 = nn.Linear(32, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    # send x through the network\n",
        "     #state=torch.from_numpy(state)\n",
        "     state.unsqueeze(0)\n",
        "     x = torch.relu(self.lvl1(state))\n",
        "     x = torch.relu(self.lvl2(x))\n",
        "     x = torch.softmax(self.lvl3(x), dim=-1)#i choosed this because the action space is discerete\n",
        "\n",
        "     return x\n",
        "\n",
        "  def select_action(self, state):\n",
        "        action_values = self.forward(state)\n",
        "        action = torch.argmin(action_values, dim=-1).item()\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0X6I3KnQ0-G4"
      },
      "source": [
        "### DQN Agent\n",
        "----\n",
        "We will be using experience replay memory for training our model.\n",
        "An Agent's memory is as important as its model, and will be another attribute of our agent.\n",
        "Other appropriate attributes are the hyperparameters (gamma, lr, etc.).\n",
        "Give the agent a replay method that trains on a batch from its memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "isfZ6WCU0-G6"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.function_base import iterable\n",
        "\n",
        "import random\n",
        "class Agent:\n",
        "    def __init__(self, observation_size, action_size):\n",
        "        self.observation_size=observation_size\n",
        "        self.action_size = action_size\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.model = Model(observation_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "        self.maxlen=200\n",
        "        self.memory =deque(maxlen=self.maxlen)  # memory that stores N most new transitions\n",
        "        # good place to store hyperparameters as attributes\n",
        "        self.batch_size = 20\n",
        "        self.gamma = 0.6\n",
        "        self.learning_rate = 0.5\n",
        "        self.epsilon=0.2\n",
        "\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # add to memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.maxlen:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def act(self, state):\n",
        "        # return an action from the model\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_size - 1)\n",
        "        else:\n",
        "            state = torch.tensor(state, dtype=torch.float32)\n",
        "            q_values = self.model(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        # update model based on replay memory\n",
        "        # you might want to make a self.train() helper method\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "        current_qv = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "        next_qv = self.model(next_states).max(dim=1, keepdim=True)[0]\n",
        "        target_qv = rewards + (1 - dones) * self.gamma * next_qv\n",
        "        loss = self.criterion(current_qv, target_qv)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4pM9mBrn0-G7"
      },
      "source": [
        "### Main Training loop\n",
        "---\n",
        "Make a function that takes and environment and an agent, and runs through $n$ episodes.\n",
        "Remember to call that agent's replay function to learn from its past (once it has a past).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xgQwDsIJ0-G8"
      },
      "outputs": [],
      "source": [
        "def train(env, agent, episodes=1000, batch_size=64):  # train for many games\n",
        "     for _ in tqdm(range(episodes)):\n",
        "         state = env.reset()\n",
        "         done = False\n",
        "         while not done:\n",
        "             # 1. make a move in game.\n",
        "             action = agent.act(state)\n",
        "             next_state, reward, done, _ = env.step(action)\n",
        "             # 2. have the agent remember stuff.\n",
        "             agent.remember(state, action, reward, next_state, done)\n",
        "             # 3. update state\n",
        "             state = next_state\n",
        "             # 4. if we have enough experiences in out memory, learn from a batch with replay.\n",
        "             if len(agent.memory) >= batch_size:\n",
        "                 agent.replay(batch_size)\n",
        "     env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "WQ_A_OmF0-G8"
      },
      "source": [
        "### Putting it together\n",
        "---\n",
        "We train an agent on the environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z26fufef0-G9",
        "outputId": "32cf557d-79c6-4e20-d66e-d406c18df22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:00<01:43,  9.63it/s]<ipython-input-4-380812cffba3>:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([20, 20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "100%|██████████| 1000/1000 [00:25<00:00, 38.90it/s]\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')  # , render_mode='human')\n",
        "agent = Agent(env.observation_space.shape[0], env.action_space.n)\n",
        "train(env, agent)\n",
        "torch.save(agent.model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gAWWe0_p0-G9"
      },
      "source": [
        "## Optional (highly recommended): Atari\n",
        "Adapt your agent to play an Atari game of your choice.\n",
        "https://www.gymlibrary.dev/environments/atari/air_raid/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "37rkpOAP0-G-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}